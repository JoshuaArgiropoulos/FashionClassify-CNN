{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b15350-5897-41eb-85c4-9437122be46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Breakdown of the simple CNN\n",
    "\n",
    "# Convolutional Layers \n",
    "\n",
    "# These layers are the core of the CNN, designed to learn hierarchical feature representations of the input images. \n",
    "#The first convolutional layer starts with RGB images and increases the depth (32, 64, 128). \n",
    "#This increase in depth allows the network to learn more complex features at each level, \n",
    "#starting from basic edges and textures to more complex patterns.\n",
    "# Each convolutional layer uses a kernel size of 3 with padding set to 1, which helps maintain the \n",
    "#spatial dimensions of the output while allowing the network to learn spatial hierarchies in the data.\n",
    "# The stride is kept at 1 to process the image in fine steps, capturing detailed features without losing much information between layers.\n",
    "\n",
    "# Pooling Layer\n",
    "\n",
    "# Max pooling is used to reduce the spatial dimensions of the output from the convolutional layers. \n",
    "#By using a 2x2 window with a stride of 2, it reduces the height and width of the feature maps by half, \n",
    "#decreasing the amount of computation needed in deeper layers and controlling overfitting\n",
    "\n",
    "# Fully Connected Layers\n",
    "\n",
    "# After the convolutional and pooling layers have extracted and condensed the features from the input image, \n",
    "#the fully connected layers are used to perform classification based on those features.\n",
    "# The first fully connected layer reduces the dimensionality from the flattened convolutional layer\n",
    "#outputs to a smaller feature space, before connecting to the final layer which outputs the predictions across the # of classes\n",
    "\n",
    "# RELU\n",
    "\n",
    "# ReLU used after each convolutional and fully connected layer \n",
    "#except the last one to introduce non-linearity into the model, allowing it to learn complex patterns. \n",
    "#ReLU is used to mitigate the vanishing gradient problem compared to other activation functions like sigmoid or tanh.\n",
    "\n",
    "# Dropout\n",
    "\n",
    "# Dropout is used to prevent overfitting by randomly setting a fraction of input units to 0 at each update during training time, \n",
    "#which helps by preventing it from relying too much on any one node.\n",
    "# The dropout layer is specifically placed after the first fully connected layer to regularize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d8ab2e3-0f5b-48ce-b00a-57846f82d52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda\")\n",
    "else:\n",
    "    print(\"CPU\")\n",
    "\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# For storing the loss and accuracy to plot later\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "\n",
    "\n",
    "train_csv_path = 'train.csv'\n",
    "test_csv_path = 'test.csv'\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv(train_csv_path, delimiter='\\t', skipinitialspace=True)\n",
    "test_df = pd.read_csv(test_csv_path, delimiter='\\t', skipinitialspace=True)\n",
    "\n",
    "# Define the dataset class\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.label_mapping = {label: idx for idx, label in enumerate(dataframe.iloc[:, 1].unique())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Assuming image ids are in the first column\n",
    "        img_id = self.dataframe.iloc[idx, 0]\n",
    "        img_name = os.path.join(self.image_dir, f\"{img_id}.jpg\")\n",
    "        image = Image.open(img_name).convert('RGB')  # Ensure the image is loaded in RGB mode\n",
    "        label_name = self.dataframe.iloc[idx, 1]\n",
    "        label = self.label_mapping[label_name]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Ensure the image has 3 channels\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image at index {idx} does not have 3 channels after transformation.\")\n",
    "    \n",
    "        return image, label\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    # transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "     \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# Create the dataset\n",
    "train_dataset = FashionDataset(dataframe=train_df, image_dir='archive/images', transform=transform)\n",
    "test_dataset = FashionDataset(dataframe=test_df, image_dir='archive/images', transform=transform)\n",
    "\n",
    "# Create the dataloaders\n",
    "batch_size = 32\n",
    "validation_ratio = 0.1\n",
    "num_train_examples = len(train_dataset)\n",
    "num_validation_examples = int(num_train_examples * validation_ratio)\n",
    "num_train_examples -= num_validation_examples\n",
    "train_subset, validation_subset = random_split(train_dataset, [num_train_examples, num_validation_examples])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "validation_loader = DataLoader(validation_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # Adjust dropout rate as needed\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 128 * 16 * 16)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the model\n",
    "num_classes = 13  \n",
    "model = SimpleCNN(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function for the training step\n",
    "def train(model, criterion, optimizer, dataloader, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "        # Move tensors to the configured device\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Train loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "def validate(model, criterion, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No need to track the gradients\n",
    "        for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Validation loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "# Function to evaluate the model on the test set\n",
    "def test(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in dataloader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46d491ef-fbce-433d-aefd-48c393781aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train loss: 0.4434, Accuracy: 86.71%\n",
      "Validation loss: 0.2252, Accuracy: 92.93%\n",
      "Epoch 2/10\n",
      "Train loss: 0.2163, Accuracy: 93.59%\n",
      "Validation loss: 0.1839, Accuracy: 94.36%\n",
      "Epoch 3/10\n",
      "Train loss: 0.1556, Accuracy: 95.29%\n",
      "Validation loss: 0.1669, Accuracy: 94.96%\n",
      "Epoch 4/10\n",
      "Train loss: 0.1218, Accuracy: 96.32%\n",
      "Validation loss: 0.1788, Accuracy: 95.10%\n",
      "Epoch 5/10\n",
      "Train loss: 0.0902, Accuracy: 97.12%\n",
      "Validation loss: 0.2118, Accuracy: 94.93%\n",
      "Epoch 6/10\n",
      "Train loss: 0.0749, Accuracy: 97.58%\n",
      "Validation loss: 0.1960, Accuracy: 94.91%\n",
      "Epoch 7/10\n",
      "Train loss: 0.0628, Accuracy: 97.96%\n",
      "Validation loss: 0.1902, Accuracy: 95.82%\n",
      "Epoch 8/10\n",
      "Train loss: 0.0499, Accuracy: 98.43%\n",
      "Validation loss: 0.2141, Accuracy: 95.15%\n",
      "Epoch 9/10\n",
      "Train loss: 0.0483, Accuracy: 98.45%\n",
      "Validation loss: 0.2453, Accuracy: 95.75%\n",
      "Epoch 10/10\n",
      "Train loss: 0.0423, Accuracy: 98.65%\n",
      "Validation loss: 0.2618, Accuracy: 95.80%\n"
     ]
    }
   ],
   "source": [
    "# Function for the validation step\n",
    "\n",
    "# Add validation to the training epochs\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    train(model, criterion, optimizer, train_loader, device)\n",
    "    validate(model, criterion, validation_loader, device)\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'fashion_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0308035-c97e-49c3-877f-0b971ce7cdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 28.93%\n"
     ]
    }
   ],
   "source": [
    "# Load the model (for evaluation)\n",
    "model.load_state_dict(torch.load('fashion_model.pth'))\n",
    "\n",
    "# Evaluate the model\n",
    "test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291e08e8-2e1d-4606-a41c-59cea5d95eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model Performance Conclusion\n",
    "\n",
    "# The baseline approach's performance on the training and validation datasets shows a consistent improvement over the epochs, which is a a great indication that the model is learning\n",
    "\n",
    "# Training Performance\n",
    "\n",
    "#The model's accuracy on the training set shows an upward trend from 86.71% to 98.65% over 10 epochs. \n",
    "#Similarly, the training loss decreased from 0.4434 to 0.0423. \n",
    "#This suggests that the model is fitting well to the training data and learning the underlying patterns effectively.\n",
    "\n",
    "# Validation Performance\n",
    "\n",
    "#The validation accuracy started at a high of 92.93% and increased to 95.80% by the 10th epoch, \n",
    "#while the validation loss initially decreased but then started to rise from the 4th epoch onward, reaching 0.2618. \n",
    "#The increasing loss alongside increasing accuracy may indicate that while the model is getting better at classifying correctly, \n",
    "#it is also becoming more confident in its incorrect predictions, which could be a sign of overfitting.\n",
    "\n",
    "# Test Performance\n",
    "\n",
    "#significantly lower test accuracy of 28.93%. This discrepancy is indicative of a model that has not generalized well to unseen data.\n",
    "\n",
    "# Results\n",
    "\n",
    "# Overfitting: The improving training accuracy and decreasing loss are usually good signs\n",
    "#the increasing validation loss suggests the model might be overfitting. \n",
    "#Overfitting occurs when a model learns the details and noise in the training data to an extent that it \n",
    "#negatively impacts the model's performance on new data.\n",
    "\n",
    "#The significant difference between validation accuracy and test accuracy is concerning. \n",
    "#This could be due to a variety of factors such as\n",
    "\n",
    "# A difference in the distribution of data between training/validation and testing datasets - unlikly given how the assignment was set up\n",
    "# The model might have memorized specific features of the training set that aren't as prevalent or are represented differently in the test set.\n",
    "# There might be a data leakage in the training/validation set, or \n",
    "#the test set could be too challenging or not well-represented by the model's learned features.\n",
    "\n",
    "#Given the high performance on the training set and the lower performance on the test set, \n",
    "#it's clear that the model could benefit from better regularization techniques. \n",
    "#Regularization can help to reduce overfitting by penalizing overly complex models and \n",
    "#promoting simpler hypotheses that may generalize better to unseen data.\n",
    "\n",
    "# Data Augmentation or Additional Data: To help the model generalize better, - which will be done in later part of the assignment\n",
    "\n",
    "# In conclusion, while the model shows promise in learning from the training data, \n",
    "#the poor test accuracy highlights a need for strategies to improve generalization, \n",
    " \n",
    "#and ensuring that the model has not overfit to the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ace29-2792-445a-b480-2fa76d47b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Mode - Adjusting Dropout Rate\n",
    "# Why Tune? \n",
    "#The dropout rate determines how much information is \"forgotten\" during a training iteration. \n",
    "#Finding the right balance is key to achieving good generalization without losing necessary information.\n",
    "\n",
    "# I think the model is overfitting so i will try increasing the dropout rate.\n",
    "\n",
    "#Hopefully this will help control my overfitting issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2116b71-c542-4d44-8b66-0fca4df02563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda\")\n",
    "else:\n",
    "    print(\"CPU\")\n",
    "\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# For storing the loss and accuracy to plot later\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "\n",
    "\n",
    "train_csv_path = 'train.csv'\n",
    "test_csv_path = 'test.csv'\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv(train_csv_path, delimiter='\\t', skipinitialspace=True)\n",
    "test_df = pd.read_csv(test_csv_path, delimiter='\\t', skipinitialspace=True)\n",
    "\n",
    "# Define the dataset class\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.label_mapping = {label: idx for idx, label in enumerate(dataframe.iloc[:, 1].unique())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Assuming image ids are in the first column\n",
    "        img_id = self.dataframe.iloc[idx, 0]\n",
    "        img_name = os.path.join(self.image_dir, f\"{img_id}.jpg\")\n",
    "        image = Image.open(img_name).convert('RGB')  # Ensure the image is loaded in RGB mode\n",
    "        label_name = self.dataframe.iloc[idx, 1]\n",
    "        label = self.label_mapping[label_name]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Ensure the image has 3 channels\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image at index {idx} does not have 3 channels after transformation.\")\n",
    "    \n",
    "        return image, label\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    # transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "     \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# Create the dataset\n",
    "train_dataset = FashionDataset(dataframe=train_df, image_dir='archive/images', transform=transform)\n",
    "test_dataset = FashionDataset(dataframe=test_df, image_dir='archive/images', transform=transform)\n",
    "\n",
    "# Create the dataloaders\n",
    "batch_size = 32\n",
    "validation_ratio = 0.1\n",
    "num_train_examples = len(train_dataset)\n",
    "num_validation_examples = int(num_train_examples * validation_ratio)\n",
    "num_train_examples -= num_validation_examples\n",
    "train_subset, validation_subset = random_split(train_dataset, [num_train_examples, num_validation_examples])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "validation_loader = DataLoader(validation_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class DropoutModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DropoutModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.7)  # Adjust dropout rate as needed\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 128 * 16 * 16)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the model\n",
    "num_classes = 13  \n",
    "model = DropoutModel(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function for the training step\n",
    "def train(model, criterion, optimizer, dataloader, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "        # Move tensors to the configured device\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Train loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "def validate(model, criterion, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No need to track the gradients\n",
    "        for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Validation loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "# Function to evaluate the model on the test set\n",
    "def test(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in dataloader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d6aec4b-cfd5-4bd5-b469-caec5ac785a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train loss: 0.4896, Accuracy: 85.18%\n",
      "Validation loss: 0.2819, Accuracy: 92.31%\n",
      "Epoch 2/10\n",
      "Train loss: 0.2638, Accuracy: 92.12%\n",
      "Validation loss: 0.2205, Accuracy: 93.62%\n",
      "Epoch 3/10\n",
      "Train loss: 0.2058, Accuracy: 94.02%\n",
      "Validation loss: 0.1909, Accuracy: 94.44%\n",
      "Epoch 4/10\n",
      "Train loss: 0.1638, Accuracy: 95.10%\n",
      "Validation loss: 0.1988, Accuracy: 94.56%\n",
      "Epoch 5/10\n",
      "Train loss: 0.1337, Accuracy: 95.90%\n",
      "Validation loss: 0.2190, Accuracy: 94.31%\n",
      "Epoch 6/10\n",
      "Train loss: 0.1199, Accuracy: 96.30%\n",
      "Validation loss: 0.1824, Accuracy: 95.20%\n",
      "Epoch 7/10\n",
      "Train loss: 0.1029, Accuracy: 96.80%\n",
      "Validation loss: 0.1821, Accuracy: 95.47%\n",
      "Epoch 8/10\n",
      "Train loss: 0.0864, Accuracy: 97.36%\n",
      "Validation loss: 0.2131, Accuracy: 95.08%\n",
      "Epoch 9/10\n",
      "Train loss: 0.0794, Accuracy: 97.51%\n",
      "Validation loss: 0.2118, Accuracy: 95.72%\n",
      "Epoch 10/10\n",
      "Train loss: 0.0727, Accuracy: 97.80%\n",
      "Validation loss: 0.2068, Accuracy: 95.18%\n"
     ]
    }
   ],
   "source": [
    "# Function for the validation step\n",
    "\n",
    "# Add validation to the training epochs\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    train(model, criterion, optimizer, train_loader, device)\n",
    "    validate(model, criterion, validation_loader, device)\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'fashion_dropout.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8857c97f-1a67-4434-a7bc-064a5a90dd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 28.57%\n"
     ]
    }
   ],
   "source": [
    "# Load the model (for evaluation)\n",
    "model.load_state_dict(torch.load('fashion_dropout.pth'))\n",
    "\n",
    "# Evaluate the model\n",
    "test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30691884-c413-4f52-90de-feabfad19d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout Model Performance Conclusion\n",
    "\n",
    "#aimed to address overfitting observed in the baseline model by regularizing the network during training. \n",
    "\n",
    "# Training and Validation Performance\n",
    "# Decrease in Training Loss: The model shows a consistent decrease in training loss from 0.4896 to 0.0727 over 10 epochs, \n",
    "#indicating that the model is learning effectively from the training data.\n",
    "\n",
    "# Increase in Training Accuracy:\n",
    "#Similarly, training accuracy improves from 85.18% to 97.80%, \n",
    "#demonstrating the model's increasing proficiency in classifying the training data correctly.\n",
    "\n",
    "# Validation Loss and Accuracy Fluctuations: \n",
    "#The validation loss decreases initially but shows fluctuations in later epochs, \n",
    "#with an increase in epochs 5, 8, and 9 compared to earlier epochs. Despite these fluctuations, \n",
    "#the validation accuracy generally remains high, peaking at 95.72% in epoch 9, \n",
    "#suggesting that the model retains a good level of generalization on unseen data.\n",
    "\n",
    "# Analysis of Dropout Impact\n",
    "# The implemented dropout appears to have a positive impact on the model's ability to generalize, as indicated by high validation accuracy. \n",
    "#However, the fluctuating validation loss suggests that the model's confidence in its \n",
    "#predictions varies significantly across epochs. This could be an indicator of the model still \n",
    "#overfitting to some extent or being affected by the increased regularization in a way that impacts its loss metrics more than its accuracy.\n",
    "\n",
    "# Test Performance\n",
    "# Significant Discrepancy: There is a stark discrepancy between the validation accuracy and test accuracy, \n",
    "#with the latter being significantly lower at 28.57%. This huge gap indicates a problem with model generalization to the test set.\n",
    "\n",
    "# Conclusions and Recommendations\n",
    "#Increasing the dropout hurt the models performence but the use of dropout helped the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d23defd-fc06-4db3-b3bb-4c4e17b29ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda\")\n",
    "else:\n",
    "    print(\"CPU\")\n",
    "\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# For storing the loss and accuracy to plot later\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "\n",
    "\n",
    "train_csv_path = 'train.csv'\n",
    "test_csv_path = 'test.csv'\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv(train_csv_path, delimiter='\\t', skipinitialspace=True)\n",
    "test_df = pd.read_csv(test_csv_path, delimiter='\\t', skipinitialspace=True)\n",
    "\n",
    "# Define the dataset class\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.label_mapping = {label: idx for idx, label in enumerate(dataframe.iloc[:, 1].unique())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Assuming image ids are in the first column\n",
    "        img_id = self.dataframe.iloc[idx, 0]\n",
    "        img_name = os.path.join(self.image_dir, f\"{img_id}.jpg\")\n",
    "        image = Image.open(img_name).convert('RGB')  # Ensure the image is loaded in RGB mode\n",
    "        label_name = self.dataframe.iloc[idx, 1]\n",
    "        label = self.label_mapping[label_name]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Ensure the image has 3 channels\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image at index {idx} does not have 3 channels after transformation.\")\n",
    "    \n",
    "        return image, label\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    # transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "     \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n",
    "    transforms.RandomRotation(10),  # Randomly rotate images by up to 10 degrees\n",
    "    transforms.RandomResizedCrop(128, scale=(0.8, 1.0)),  # Randomly crop and resize images\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Randomly change brightness, contrast, and saturation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# Create the dataset\n",
    "train_dataset = FashionDataset(dataframe=train_df, image_dir='archive/images', transform=train_transforms)\n",
    "test_dataset = FashionDataset(dataframe=test_df, image_dir='archive/images', transform=transform)\n",
    "\n",
    "# Create the dataloaders\n",
    "batch_size = 32\n",
    "validation_ratio = 0.1\n",
    "num_train_examples = len(train_dataset)\n",
    "num_validation_examples = int(num_train_examples * validation_ratio)\n",
    "num_train_examples -= num_validation_examples\n",
    "train_subset, validation_subset = random_split(train_dataset, [num_train_examples, num_validation_examples])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "validation_loader = DataLoader(validation_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 128 * 16 * 16)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the model\n",
    "num_classes = 13  \n",
    "model = SimpleCNN(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function for the training step\n",
    "def train(model, criterion, optimizer, dataloader, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "        # Move tensors to the configured device\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Train loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "def validate(model, criterion, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No need to track the gradients\n",
    "        for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Validation loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "# Function to evaluate the model on the test set\n",
    "def test(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in dataloader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91e7f3aa-b18b-4f88-bd21-13d1719a522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train loss: 0.7395, Accuracy: 76.48%\n",
      "Validation loss: 0.4363, Accuracy: 86.92%\n",
      "Epoch 2/10\n",
      "Train loss: 0.4468, Accuracy: 86.02%\n",
      "Validation loss: 0.3411, Accuracy: 89.59%\n",
      "Epoch 3/10\n",
      "Train loss: 0.3706, Accuracy: 88.20%\n",
      "Validation loss: 0.2945, Accuracy: 90.85%\n",
      "Epoch 4/10\n",
      "Train loss: 0.3389, Accuracy: 89.45%\n",
      "Validation loss: 0.2808, Accuracy: 90.95%\n",
      "Epoch 5/10\n",
      "Train loss: 0.3168, Accuracy: 90.21%\n",
      "Validation loss: 0.2829, Accuracy: 91.27%\n",
      "Epoch 6/10\n",
      "Train loss: 0.2995, Accuracy: 90.83%\n",
      "Validation loss: 0.2689, Accuracy: 91.67%\n",
      "Epoch 7/10\n",
      "Train loss: 0.2896, Accuracy: 91.05%\n",
      "Validation loss: 0.2618, Accuracy: 92.11%\n",
      "Epoch 8/10\n",
      "Train loss: 0.2660, Accuracy: 91.66%\n",
      "Validation loss: 0.2418, Accuracy: 93.05%\n",
      "Epoch 9/10\n",
      "Train loss: 0.2592, Accuracy: 91.82%\n",
      "Validation loss: 0.2212, Accuracy: 92.90%\n",
      "Epoch 10/10\n",
      "Train loss: 0.2579, Accuracy: 91.81%\n",
      "Validation loss: 0.2250, Accuracy: 93.03%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    train(model, criterion, optimizer, train_loader, device)\n",
    "    validate(model, criterion, validation_loader, device)\n",
    "\n",
    "torch.save(model.state_dict(), 'fashion_Aug.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecee7197-7994-4bcb-9e64-1d4b466d9b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 29.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(torch.load('fashion_Aug.pth'))\n",
    "\n",
    "test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e274e-7a99-4ecf-bfa0-1b85e23c0511",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conclusion on the Performance - Data Augmentation\n",
    "\n",
    "# The experiment results from incorporating data augmentation into the training process demonstrate both its strengths and limitations \n",
    "#in enhancing model performance. Data augmentation was intended to increase the model's ability to generalize by introducing a wider \n",
    "#variety of training examples without the need for additional data collection. \n",
    "\n",
    "# Training and Validation Performance\n",
    "# Initial Increase in Training Loss: \n",
    "\n",
    "#The first epoch starts with a higher training loss (0.7395) compared to models without data augmentation. \n",
    "#This is expected, as data augmentation introduces more variability and complexity into the training process, \n",
    "#making the initial learning phase more challenging.\n",
    "\n",
    "# Steady Improvement: \n",
    "\n",
    "#Both training loss and accuracy show consistent improvement over the epochs, with training loss decreasing from 0.7395 to 0.2579, \n",
    "#and accuracy increasing from 76.48% to 91.81%. This indicates that the model effectively learns from the augmented data over time.\n",
    "\n",
    "# Validation Performance: \n",
    "\n",
    "#The validation loss decreases from 0.4363 to 0.2250 across epochs, and validation accuracy improves, reaching a peak of 93.05% in epoch 8 before \n",
    "#stabilizing around 93%. The improvement in validation metrics suggests that data augmentation has helped enhance the model's generalization \n",
    "#capabilities.\n",
    "\n",
    "# Test Performance\n",
    "# Marginal Improvement: \n",
    "\n",
    "#The test accuracy stands at 29.00%, which, while slightly better than the dropout model's performance (28.57%), still indicates a significant \n",
    "#generalization gap. This discrepancy highlights that while data augmentation and dropout regularization can mitigate overfitting to some extent,\n",
    "#additional strategies may be required to bridge the gap between training/validation performance and test set performance.\n",
    "\n",
    "# Analysis and Recommendations\n",
    "# Data Augmentation's Impact: The consistent improvement in training and validation metrics confirms the beneficial impact of data \n",
    "#augmentation on model learning and generalization. However, the marginal increase in test accuracy suggests limits to its effectiveness alone.\n",
    "\n",
    "# Generalization Gap: \n",
    "\n",
    "#The stark difference between validation and test performance suggests potential issues beyond model overfitting. \n",
    "#It could point towards a distribution shift between training/validation data and test data, or \n",
    "#it may indicate that the model's capacity is not adequately aligned with the complexity of the test data.\n",
    "\n",
    "# Further Experimentation: \n",
    "#Experimenting with different or additional data augmentation techniques could provide further insights into their impact on model\n",
    "#performance. Moreover, exploring other regularization techniques, adjusting model architecture, or using advanced training strategies \n",
    "#like transfer learning could offer additional pathways to improve test accuracy.\n",
    "\n",
    "# Review Dataset Split: \n",
    "#It's crucial to ensure the dataset is split in a manner that accurately reflects the distribution of data the \n",
    "#model will encounter in practice. A review of the splitting process may reveal opportunities for improvement.\n",
    "\n",
    "# In conclusion, while data augmentation has positively impacted training dynamics and validation performance, \n",
    "#bridging the significant gap to test performance requires a multifaceted approach. \n",
    "#This might include further experimentation with data processing, model architecture adjustments, and exploring additional or more sophisticated \n",
    "#regularization and augmentation strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
